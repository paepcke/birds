[Paths]

# Root of the data/test files:
root_train_test_data = /home/data/birds/recombined_data

# Only relevant if running multi-process.
# File showing the distribution of GPUs
# across machines, or within one machine.
# Allows specification of which GPU(s) to
# use on each machine
# See file world_map.json.Example for
# how to write this configuration

world_map     = ../../world_map.json

[Training]

net_name      = resnet18
min_epochs    = 15
max_epochs    = 100
batch_size    = 32
# The 'k' in k=fold cross validation:
num_folds     = 10
seed          = 42
kernel_size   = 7
lr            = 0.001
momentum      = 0.9
# Training images should be scaled to (in pixels):
sample_width  = 400
sample_height = 400 
verbose       = yes
# Number of seconds between printing status
# to the console, if verbose is 'yes':
show_alive    = 30

[Parallelism]

# Must be the same on all collaborating
# machines:

seed          = 42

# Communication used for inter process/machine
# communication during parallel training.
# Any port is OK, one could use 29920, which
# is registered as a Nintendo wifi port.
# 5678 is the default pytorch port:

master_port = 5678
