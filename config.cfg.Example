[Paths]

root_train_test_data = /home/data/birds/recombined_data/*

[Training]

net_name      = resnet18
min_epochs    = 15
max_epochs    = 100
batch_size    = 32
# The 'k' in k=fold cross validation:
num_folds     = 10
seed          = 42
kernel_size   = 7
lr            = 0.001
momentum      = 0.9
# Training images should be scaled to (in pixels):
sample_width  = 400
sample_height = 400 
verbose       = yes
# Number of seconds between printing status
# to the console, if verbose is 'yes':
show_alive    = 30

[Parallelism]

# Must be the same on all collaborating
# machines:

seed          = 42

# Num of training processes running.
# On each machine: One process per GPU
# on that machine. Or 1 if only CPU is
# available.
#
#    foo.bar.com  = 4
#    127.0.0.1    = 5
#    localhost    = 3
#    172.12.145.1 = 6
#  
# The config parser identifies which of the entries is
# 'localhost' by comparing against local hostname.
# Though 'localhost' or '127.0.0.1' may be provided
# explicitly:

quatro.stanford.edu     = 3
quintus.stanford.edu    = 2

# Communication used for inter process/machine
# communication during parallel training.
# Any port is OK, this is the one registered as
# a Nintendo wifi port:

# If this port is modified, ensure that
# all participating machine's corresponding
# entries are changed to the same new
# port. Collaborative processes communicate
# via this port. Can be anything available.

pytorch_comm_port       = 29920
