[Paths]

# For testing this config file is in the
# same dir as the test .py file:

root_train_test_data = ./data/*

[Training]

net_name      = resnet18
min_epochs    = 15
max_epochs    = 100
batch_size    = 2
# The 'k' in k=fold cross validation:
num_folds     = 3
seed          = 42
kernel_size   = 7
sample_width  = 400
sample_height = 400
lr            = 0.002


[Parallelism]

# Num of training processes running.
# On each machine: One process per GPU
# on that machine. Or 1 if only CPU is
# available.
#
#    foo.bar.com  = 4
#    127.0.0.1    = 5
#    localhost    = 3
#    172.12.145.1 = 6
#  
# The config parser identifies which of the entries is
# 'localhost' by comparing against local hostname.
# Though 'localhost' or '127.0.0.1' may be provided
# explicitly:

quatro.stanford.edu     = 3
quintus.stanford.edu    = 2

# Communication used for inter process/machine
# communication during parallel training.
# Any port is OK, this is the one registered as
# a Nintendo wifi port:

pytorch_comm_port       = 29920

